{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mortezaaghajanzadeh/Machine-learning-in-Finance/blob/main/Lecture%203/introduction_to_tree_based_models_lecture_3_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 3: Tree-Based Models.**\n",
        "### Example: train bagging, boosting, and random forest models to predict historical house price growth using the MacroHistory database: https://www.macrohistory.net/database/."
      ],
      "metadata": {
        "id": "uTxaRXhp98WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages.\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "_uzhAUPOBf9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load packages.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "uhwSFvGULeCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data.\n",
        "file_path = 'https://www.dropbox.com/scl/fi/3kcanu4zfvr7it0wiqnb3/JSTdatasetR3.xlsx?rlkey=an6g1cqxicz0005g4fp1ggv21&dl=1'\n",
        "data = pd.read_excel(file_path, sheet_name='Data')"
      ],
      "metadata": {
        "id": "WEoXDeS3pIHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transform variables.**"
      ],
      "metadata": {
        "id": "qSftElV-vyQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Business loans to GDP ratio.\n",
        "data['tbus_to_gdp'] = data['tbus'] / data['gdp']\n",
        "\n",
        "# Household loans to GDP ratio.\n",
        "data['thh_to_gdp'] = data['thh'] / data['gdp']\n",
        "\n",
        "# Mortgage loans to non-financial firms in private sector to GDP ratio.\n",
        "data['tmort_to_gdp'] = data['tmort'] / data['gdp']\n",
        "\n",
        "# Total loans to non-financial firms in private sector to GDP ratio.\n",
        "data['tloans_to_gdp'] = data['tloans'] / data['gdp']\n",
        "\n",
        "# Net exports to GDP ratio.\n",
        "data['net_exports_to_gdp'] = (data['exports'] - data['imports']) / data['gdp']\n",
        "\n",
        "# Government surplus to GDP ratio.\n",
        "data['surplus_to_gdp'] = (data['revenue'] - data['expenditure']) / data['gdp']\n",
        "\n",
        "# Calculate Inflation (Percentage change in price index)\n",
        "data['stock_price_growth'] = data.groupby('country')['stocks'].pct_change() * 100\n",
        "\n",
        "# Calculate Inflation (Percentage change in price index)\n",
        "data['inflation'] = data.groupby('country')['cpi'].pct_change() * 100\n",
        "\n",
        "# Calculate House Price Growth (Percentage change in house prices)\n",
        "data['hpnom'] = data.groupby('country')['hpnom'].pct_change() * 100\n",
        "\n",
        "# Shift crisis variable ahead one period.\n",
        "data['hpnom_growth_lead'] = data.groupby('country')['hpnom'].shift(-1)\n",
        "\n",
        "# Drop missing values.\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Drop data prior to 1970.\n",
        "data = data[data['year'] >= 1970].copy()"
      ],
      "metadata": {
        "id": "hjcnp-CSs-aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize features and target.**"
      ],
      "metadata": {
        "id": "bOzbf3018a0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot house price growth.\n",
        "sns.histplot(data['hpnom_growth_lead'], kde=True)\n",
        "plt.title('Distribution of House Price Growth')\n",
        "plt.xlabel('House Price Growth (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gqM0d8L28eks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting house price growth time series for selected countries.\n",
        "sample_countries = data['country'].unique()[0:10]\n",
        "for country in sample_countries:\n",
        "    subset = data[data['country'] == country]\n",
        "    plt.plot(subset['year'], subset['hpnom_growth_lead'], label=country)\n",
        "\n",
        "# Set plot labels and legend.\n",
        "plt.title('House Price Growth Over Time by Country')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('House Price Growth (%)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "__Zd3rkf8yNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define features and target.**"
      ],
      "metadata": {
        "id": "_r5O0YdA0ZsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features.\n",
        "X = data.drop(columns=['crisisJST',\n",
        "                       'country',\n",
        "                       'ifs',\n",
        "                       'iso',\n",
        "                       'year',\n",
        "                       'pop',\n",
        "                       'rgdpmad',\n",
        "                       'rgdppc',\n",
        "                       'rconpc',\n",
        "                       'gdp',\n",
        "                       'iy',\n",
        "                       'cpi',\n",
        "                       'stocks',\n",
        "                       'ca',\n",
        "                       'imports',\n",
        "                       'exports',\n",
        "                       'narrowm',\n",
        "                       'money',\n",
        "                       'revenue',\n",
        "                       'expenditure',\n",
        "                       'xrusd',\n",
        "                       'tloans',\n",
        "                       'tmort',\n",
        "                       'thh',\n",
        "                       'tbus',\n",
        "                       'hpnom',\n",
        "                       'hpnom_growth_lead'])\n",
        "\n",
        "# Define target.\n",
        "y = data['hpnom_growth_lead']"
      ],
      "metadata": {
        "id": "wOz4iv-epI6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train models.**"
      ],
      "metadata": {
        "id": "EtWxa0HD9itt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=103)"
      ],
      "metadata": {
        "id": "QTQuP2ZBpL5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate random forest model.\n",
        "rf_model = RandomForestRegressor(n_estimators=102,\n",
        "                                 random_state=103)\n",
        "\n",
        "# Train random forest.\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xJiJJuoIpN0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate gradient boosting model.\n",
        "gbt_model = GradientBoostingRegressor(n_estimators=100,\n",
        "                                      learning_rate=0.1,\n",
        "                                      random_state=103)\n",
        "\n",
        "# Train gradient boosting model.\n",
        "gbt_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ticVHdPlrSfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate bagging model.\n",
        "bag_model = BaggingRegressor(n_estimators=100,\n",
        "                             random_state=102)\n",
        "\n",
        "# Train bagging model.\n",
        "bag_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "uJxvPhmOpR74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize training process.**"
      ],
      "metadata": {
        "id": "mYa4kgvf9obL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store MSE from each step.\n",
        "test_score = [mean_squared_error(y_test, y_pred) for y_pred in gbt_model.staged_predict(X_test)]\n",
        "\n",
        "# Plot error as function of trees.\n",
        "plt.plot(test_score)\n",
        "plt.title('Gradient Boosting Model Error')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X0EH8hq09n_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate models.**"
      ],
      "metadata": {
        "id": "9DJXHMIg9k5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to evaluate model performance.\n",
        "def evaluate_regression_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return mse, mae, r2\n",
        "\n",
        "# Evaluate model.\n",
        "rf_mse, rf_mae, rf_r2 = evaluate_regression_model(rf_model, X_test, y_test)\n",
        "gbt_mse, gbt_mae, gbt_r2 = evaluate_regression_model(gbt_model, X_test, y_test)\n",
        "bag_mse, bag_mae, bag_r2 = evaluate_regression_model(bag_model, X_test, y_test)\n",
        "\n",
        "# Print evaluation metrics.\n",
        "print(\"Random Forest Regressor - MSE:\", rf_mse, \"MAE:\", rf_mae, \"R2:\", rf_r2)\n",
        "print(\"Gradient Boosting Regressor - MSE:\", gbt_mse, \"MAE:\", gbt_mae, \"R2:\", gbt_r2)\n",
        "print(\"Bagging Regressor - MSE:\", bag_mse, \"MAE:\", bag_mae, \"R2:\", bag_r2)"
      ],
      "metadata": {
        "id": "IM60AHIGpVJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tuning models.**"
      ],
      "metadata": {
        "id": "MgiGMIoUADsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bagging."
      ],
      "metadata": {
        "id": "myApsZhLA1Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters.\n",
        "param_set_1 = {'n_estimators': 100, 'max_samples': 1.0, 'max_features': 1.0}\n",
        "param_set_2 = {'n_estimators': 150, 'max_samples': 0.8, 'max_features': 0.8}\n",
        "\n",
        "# Instantiate models.\n",
        "bag_model_1 = BaggingRegressor(**param_set_1, random_state=103)\n",
        "bag_model_2 = BaggingRegressor(**param_set_2, random_state=103)\n",
        "\n",
        "# Train models.\n",
        "bag_model_1.fit(X_train, y_train)\n",
        "bag_model_2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Bb1N-POKA3ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest."
      ],
      "metadata": {
        "id": "tducktAdAP35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters.\n",
        "param_set_1 = {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
        "param_set_2 = {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2}\n",
        "\n",
        "# Instantiate models.\n",
        "rf_model_1 = RandomForestRegressor(**param_set_1, random_state=103)\n",
        "rf_model_2 = RandomForestRegressor(**param_set_2, random_state=103)\n",
        "\n",
        "# Train models.\n",
        "rf_model_1.fit(X_train, y_train)\n",
        "rf_model_2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-UpLg6wAAHMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient boosting."
      ],
      "metadata": {
        "id": "wrgdggxVAaPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters.\n",
        "param_set_1 = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}\n",
        "param_set_2 = {'n_estimators': 150, 'learning_rate': 0.05, 'max_depth': 5}\n",
        "\n",
        "# Instantiate models.\n",
        "gbt_model_1 = GradientBoostingRegressor(**param_set_1, random_state=103)\n",
        "gbt_model_2 = GradientBoostingRegressor(**param_set_2, random_state=103)\n",
        "\n",
        "# Train models.\n",
        "gbt_model_1.fit(X_train, y_train)\n",
        "gbt_model_2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bRT6zOtJAgZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interpret feature importance using Shapley values.**"
      ],
      "metadata": {
        "id": "inN9KVPRBTe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a SHAP explainer.\n",
        "explainer = shap.TreeExplainer(gbt_model)\n",
        "\n",
        "# Calculate SHAP values.\n",
        "shap_values = explainer.shap_values(X_train)"
      ],
      "metadata": {
        "id": "Tv2hYWYqBaCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot summary of Shapley values.\n",
        "shap.summary_plot(shap_values, X_train, feature_names=X_train.columns)"
      ],
      "metadata": {
        "id": "NrHRu1aWB0jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine model output's dependence on interest rates.\n",
        "shap.dependence_plot('ltrate', shap_values, X_train)"
      ],
      "metadata": {
        "id": "ztODYIJhB3o2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}