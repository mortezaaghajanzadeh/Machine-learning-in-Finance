{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mortezaaghajanzadeh/Machine-learning-in-Finance/blob/main/Lecture%206/reinforcement_learning_lecture_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 8: Theoretical Models.**\n",
        "### Based on code from Chapter 10 in ``Machine Learning for Economics and Finance in TensorFlow 2'' (Hull, 2021)."
      ],
      "metadata": {
        "id": "uTxaRXhp98WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries.\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RH3GPpmU-KAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-1.** Define the constants and variables for the  cake-eating problem."
      ],
      "metadata": {
        "id": "w5Lycwa7-SE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define policy rule parameter.\n",
        "theta = tf.Variable(0.1, tf.float32)\n",
        "\n",
        "# Define discount factor.\n",
        "beta = tf.constant(1.0, tf.float32)\n",
        "\n",
        "# Define state at t = 0.\n",
        "s0 = tf.constant(1.0, tf.float32)"
      ],
      "metadata": {
        "id": "UG5DW7dd5kxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-2.** Define a function for the policy rule."
      ],
      "metadata": {
        "id": "WQojvs3s-daM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define policy rule.\n",
        "def policyRule(theta, s0 = s0, beta = beta):\n",
        "\ts1 = tf.clip_by_value(theta*s0,\n",
        "\tclip_value_min = 0.01, clip_value_max = 0.99)\n",
        "\treturn s1"
      ],
      "metadata": {
        "id": "j5E5gGui9UGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-3.** Define the loss function."
      ],
      "metadata": {
        "id": "gKYwm-I1-70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function.\n",
        "def loss(theta, s0 = s0, beta = beta):\n",
        "\ts1 = policyRule(theta)\n",
        "\tv1 = tf.math.log(s1)\n",
        "\tv0 = tf.math.log(s0-s1) + beta*v1\n",
        "\treturn -v0"
      ],
      "metadata": {
        "id": "eZmvxTpF9WZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-4.** Perform optimization."
      ],
      "metadata": {
        "id": "1hOJKAxZGITB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an optimizer.\n",
        "opt = tf.optimizers.Adam(0.1)\n",
        "\n",
        "# Perform minimization.\n",
        "for j in range(500):\n",
        "\topt.minimize(lambda: loss(theta),\n",
        "\tvar_list = [theta])"
      ],
      "metadata": {
        "id": "9YN2Cwi19ZYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-5.** Define an RNN model in Keras."
      ],
      "metadata": {
        "id": "vvq4U6R6GOQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define production function parameter.\n",
        "alpha = tf.constant(0.33, tf.float32)\n",
        "\n",
        "# Define discount factor.\n",
        "beta = tf.constant(0.95, tf.float32)\n",
        "\n",
        "# Define params for decision rules.\n",
        "thetaK = tf.Variable(0.1, tf.float32)\n",
        "\n",
        "# Define state grid.\n",
        "k0 = tf.linspace(0.001, 1.00, 10000)"
      ],
      "metadata": {
        "id": "YRtVQHiY9ay4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-6.** Define model parameters."
      ],
      "metadata": {
        "id": "ckX9jaWpGW21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function.\n",
        "def loss(thetaK, k0 = k0, beta = beta):\n",
        "\t# Define period t+1 capital.\n",
        "\tk1 = thetaK*k0**alpha\n",
        "\n",
        "\t# Define Euler equation residual.\n",
        "\terror = k1**alpha-beta*alpha*k0**alpha*k1**(alpha-1)\n",
        "\n",
        "\treturn tf.reduce_mean(tf.multiply(error,error))"
      ],
      "metadata": {
        "id": "hE_5fjwh9cTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-7.** Perform optimization and evaluate results."
      ],
      "metadata": {
        "id": "RAFRa4pJgcve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an optimizer.\n",
        "opt = tf.optimizers.Adam(0.1)\n",
        "\n",
        "# Perform minimization.\n",
        "for j in range(1000):\n",
        "\topt.minimize(lambda: loss(thetaK),\n",
        "\tvar_list = [thetaK])\n",
        "\n",
        "# Print thetaK.\n",
        "print(thetaK)\n",
        "\n",
        "# Compare analytical solution and thetaK.\n",
        "print(alpha*beta)"
      ],
      "metadata": {
        "id": "YR7Ay7A5gfRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-8.** Compute transition path."
      ],
      "metadata": {
        "id": "iApx9mwbgklu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial value of capital.\n",
        "k0 = 0.05\n",
        "\n",
        "# Define empty lists.\n",
        "y, k, c = [], [], []\n",
        "\n",
        "# Perform transition.\n",
        "for j in range(10):\n",
        "\t# Update variables.\n",
        "\tk1 = thetaK*k0**alpha\n",
        "\tc0 = (1-thetaK)*k0**alpha\n",
        "\n",
        "\t# Update lists.\n",
        "\ty.append(k0**alpha)\n",
        "\tk.append(k1)\n",
        "\tc.append(c0)\n",
        "\n",
        "\t# Update state.\n",
        "\tk0 = k1"
      ],
      "metadata": {
        "id": "yMgNku-Zgo--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-9.** Compute the Euler equation residuals."
      ],
      "metadata": {
        "id": "Ea_mSHWhgqq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define state grid.\n",
        "k0 = tf.linspace(0.001, 1.00, 10000)\n",
        "\n",
        "# Define function to return Euler equation residuals.\n",
        "def eer(k0, thetaK = thetaK, beta = beta):\n",
        "\t# Define period t+1 capital.\n",
        "\tk1 = thetaK*k0**alpha\n",
        "\n",
        "\t# Define Euler equation residual.\n",
        "\tresiduals = k1**alpha-beta*alpha*k0**alpha*k1**(alpha-1)\n",
        "\n",
        "\treturn residuals\n",
        "\n",
        "# Generate residuals.\n",
        "resids = eer(k0)\n",
        "\n",
        "# Print largest residual.\n",
        "print(resids.numpy().max())"
      ],
      "metadata": {
        "id": "thtIaWrOgxd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-10.** Install and import modules to perform deep Q-learning."
      ],
      "metadata": {
        "id": "2GIrEebzgzaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install keras-rl2.\n",
        "!pip install keras-rl2\n",
        "\n",
        "# Import numpy and tensorflow.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import reinforcement learning modules from keras.\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Import module for comparing RL algorithms.\n",
        "import gym"
      ],
      "metadata": {
        "id": "Hjsbt6jGtLW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-11.** Define custom reinforcement learning environment."
      ],
      "metadata": {
        "id": "yDM34wVeg4HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of capital nodes.\n",
        "n_capital = 1000\n",
        "\n",
        "# Define environment.\n",
        "class planner(gym.Env):\n",
        "\tdef __init__(self):\n",
        "\t\tself.k = np.linspace(0.01, 1.0, n_capital)\n",
        "\t\tself.action_space = \\\n",
        "\t\tgym.spaces.Discrete(n_capital)\n",
        "\t\tself.observation_space = \\\n",
        "\t\tgym.spaces.Discrete(n_capital)\n",
        "\t\tself.decision_count = 0\n",
        "\t\tself.decision_max = 100\n",
        "\t\tself.observation = 500\n",
        "\t\tself.alpha = 0.33\n",
        "\tdef step(self, action):\n",
        "\t\tassert self.action_space.contains(action)\n",
        "\t\tself.decision_count += 1\n",
        "\t\tdone = False\n",
        "\t\tif(self.observation**self.alpha-action) > 0:\n",
        "\t\t\treward = np.log(self.k[self.observation]**self.alpha-self.k[action])\n",
        "\t\telse:\n",
        "\t\t\treward = -1000\n",
        "\t\tself.observation = action\n",
        "\t\tif (self.decision_count >= self.decision_max) or reward == -1000:\n",
        "\t\t\tdone = True\n",
        "\t\treturn self.observation, reward, done,\\\n",
        "\t\t{\"decisions\": self.decision_count}\n",
        "\tdef reset(self):\n",
        "\t\tself.decision_count = 0\n",
        "\t\tself.observation = 500\n",
        "\t\treturn self.observation"
      ],
      "metadata": {
        "id": "L9-xsXDVg-st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-12.** Instantiate enviroment and define model in TensorFlow."
      ],
      "metadata": {
        "id": "jUl34j9QGlrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate planner environment.\n",
        "env = planner()\n",
        "\n",
        "# Define model in TensorFlow.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(n_capital, activation='linear'))"
      ],
      "metadata": {
        "id": "UnDdrdqFGnE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Listing 10-13.** Set model hyperparameters and train."
      ],
      "metadata": {
        "id": "DOV3gVxJGof1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify replay buffer.\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# Define policy used to make training-time decisions.\n",
        "policy = EpsGreedyQPolicy(0.30)\n",
        "\n",
        "# Define deep Q-learning network (DQN).\n",
        "dqn = DQNAgent(model=model, nb_actions=n_capital,\n",
        "\tmemory=memory, nb_steps_warmup=100,\n",
        "\tgamma=0.95, target_model_update=1e-2,\n",
        "\tpolicy=policy)\n",
        "\n",
        "# Compile and train model.\n",
        "dqn.compile(tf.keras.optimizers.Adam(0.005), metrics=['mse'])\n",
        "dqn.fit(env, nb_steps=10000)"
      ],
      "metadata": {
        "id": "sZ4EBWyLGuMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}