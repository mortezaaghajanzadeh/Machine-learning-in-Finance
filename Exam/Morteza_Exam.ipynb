{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PhD430 - Machine Learning Exam**\n",
    "## Morteza Aghajanzadeh \n",
    "### Dec 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) *Symbolic differentiation*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L & = (y - \\omega x -b)^2\\\\\n",
    "\\frac{\\partial L}{\\partial \\omega} &  = 2 (-x)(y - \\omega x -b)\\\\\n",
    "& = 2 (-2)(3 - 2 \\omega ) \\\\\n",
    "& = 8 \\omega - 12\n",
    "\\end{split}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) *The forward difference method*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L & = (y - \\omega x -b)^2\\\\\n",
    "\\frac{\\partial L}{\\partial \\omega} &  \\approx  \\dfrac{(y - (\\omega + h) x -b)^2 - (y - \\omega x -b)^2}{h}\\\\\n",
    "& \\approx \\dfrac{((y - (\\omega + h) x -b) + (y - \\omega x -b)) ((y - (\\omega + h) x -b) - (y - \\omega x -b))}{h} \\\\\n",
    "& \\approx \\dfrac{(2(y - \\omega x -b) - hx) (-hx)}{h} \\\\\n",
    "& \\approx {(2(y - \\omega x -b) - hx) (-x)}\\\\\n",
    "& \\approx {2(-x) (y - \\omega x -b)}\\\\\n",
    "& \\approx 2 (-2)(3 - 2 \\omega ) \\\\\n",
    "& = 8 \\omega - 12\n",
    "\\end{split}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) *Autodifferentiation*\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\left.\\begin{array}{c}\n",
    "g(z) = z^2 \\Rightarrow \\frac{\\partial g(z)}{\\partial z} = 2z\\\\\n",
    "f(w) = y - wx - b \\Rightarrow \\frac{\\partial f(w)}{\\partial w} = -x\n",
    "\\end{array}\\right\\} \\Rightarrow \\frac{\\partial L}{\\partial \\omega} = \\frac{\\partial g(f(w))}{\\partial f(w)} \\frac{\\partial f(w)}{\\partial \\omega} = (2f(w)) (-x) = 2 (y - \\omega x -b)(-x)  = 2 (-2)(3 - 2 \\omega ) = 8 \\omega - 12\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to estimate an AR(1) model of the log USD-GBP exchange rate:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{t} = \\alpha + \\rho y_{t-1} + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "The code in this notebook trains the model by minimizing the following loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{T}\\sum_{t=1}^{T} \\left(y_{t} - \\alpha - \\rho y_{t-1}\n",
    "\\right)^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.36203432083129883, rho: 0.2650577127933502\n",
      "loss: 0.017625970765948296\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define data path.\n",
    "file_path = 'https://www.dropbox.com/scl/fi/utj4vox9yudaj5z0ngd8d/exchange_rate.csv?rlkey=1szy4yh3x1w3pac4qds3y6hpw&dl=1'\n",
    "\n",
    "# Load data.\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified code\n",
    "I wrote than a function that do all the trainings based on the code that you provided for us.\n",
    "I set seed number in the function to get the same result every time.\n",
    "The function get the $\\alpha_{0}$, $\\rho_0$, loss_function, and opt as an input.\n",
    "I set the baseline values as given to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_model(data,α_0 = 0.05,ρ_0 = 0.05,loss_function = tf.keras.losses.mse,opt = tf.keras.optimizers.SGD()):\n",
    "\timport random\n",
    "\trandom.seed(13990508)\n",
    "\t# Convert log exchange rate to numpy array.\n",
    "\te = np.array(np.log(data['USD_GBP']))\n",
    "\n",
    "\t# Define the lagged exchange rate as a tensorflow constant.\n",
    "\tle = tf.constant(e[1:-1], tf.float32)\n",
    "\n",
    "\t# Define the exchange rate as a tensorflow constant.\n",
    "\te = tf.constant(e[2:], tf.float32)\n",
    "\t# Initialize parameters.\n",
    "\talpha = tf.Variable(α_0, tf.float32)\n",
    "\trho = tf.Variable(ρ_0, tf.float32)\n",
    "\t# Define AR(1) model to make predictions.\n",
    "\tdef ar(alpha, rho, le):\n",
    "\t\tyhat = alpha + rho*le\n",
    "\t\treturn yhat\n",
    "\t# Define loss function.\n",
    "\tdef loss(alpha, rho, e, le):\n",
    "\t\tyhat = ar(alpha, rho, le)\n",
    "\t\treturn loss_function(e, yhat)\n",
    "\t# Insantiate optimizer.\n",
    "\topt = tf.keras.optimizers.SGD()\n",
    "\t# Perform minimization.\n",
    "\tfor i in range(100):\n",
    "\t\topt.minimize(lambda:\n",
    "\t\tloss(alpha, rho, e, le),\n",
    "\t\tvar_list = [alpha, rho]\n",
    "\t\t)\n",
    "\t# Print parameters.\n",
    "\tprint('alpha: {}, rho: {}'.format(alpha.numpy(), rho.numpy()))\n",
    "\n",
    "\t# Generate predictions.\n",
    "\typred = ar(alpha, rho, le)\n",
    "\n",
    "\t# Print loss.\n",
    "\tprint('loss: {}'.format(loss(alpha, rho, e, le).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the baseline model:\n",
      "alpha: 0.36203432083129883, rho: 0.2650577127933502\n",
      "loss: 0.017625970765948296\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for the baseline model:\")\n",
    "estimation_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "Now I modify the loss function input in the defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.34546560049057007, rho: 0.306787371635437\n",
      "loss: 0.09436121582984924\n"
     ]
    }
   ],
   "source": [
    "estimation_model(data,loss_function = tf.keras.losses.mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "Now I modify the optimizer in the defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.36203432083129883, rho: 0.2650577127933502\n",
      "loss: 0.017625970765948296\n"
     ]
    }
   ],
   "source": [
    "estimation_model(data,opt=tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "Now I would use different initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.31938737630844116, rho: 0.43941545486450195\n",
      "loss: 0.010513707995414734\n"
     ]
    }
   ],
   "source": [
    "estimation_model(data,α_0 = 0.5,ρ_0 = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the change that we made in the last section has the major effect on the loss value that we get from the estimation. The initial guesses are important to provide a better estimation results for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "After calculating the initial loss value, we need to find the next guess. Gradient is helpful to provide a direction for us to find the next guess.\n",
    "The gradient is used to update $\\theta$ because it points in the direction of the steepest increase of the loss function. By moving in the opposite direction of the gradient, we can iteratively update the parameters to minimize the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "When selecting a learning rate for an optimization algorithm, it is essential to consider the trade-off between taking larger steps with each iteration and the potential for overshooting the minimum. A high learning rate can help us approach the minimum faster, but it may also lead us to miss it entirely. So, it's crucial to choose a learning rate that strikes the right balance between convergence speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "In the SDG we select a sample j uniformly form all the observations in the data and update $\\theta$ by using the \n",
    "$$\n",
    "\\theta \\coloneqq \\theta - \\alpha \\Delta_{\\theta} J^{(j)}(\\theta)\n",
    "$$\n",
    "This is the main difference between SDG and DG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "Computing the gradient of B examples simultaneously for the parameter $\\theta$ can be faster than computing B gradients separately due to hardware parallelization. So we sample B examples $j_1,\\dots, j_B$ (without replacement) form the observations and update $\\theta$ by \n",
    "$$\n",
    "\\theta \\coloneqq \\theta - \\frac{\\alpha}{B} \\sum_{k=1}^{B}\\Delta_{\\theta} J^{(j_k)}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "Due to their efficiency and effectiveness, Stochastic Gradient Descent (SGD) and Mini-Batch SGD have become popular optimization techniques for training deep learning models. Unlike regular Gradient Descent, which uses the entire dataset to calculate gradients, SGD and Mini-Batch SGD operate on random subsets of the data. This randomness not only makes computations more scalable for large datasets but also leads to faster convergence. Training deep learning models is computationally intensive, and the frequent parameter updates in SGD and Mini-Batch SGD contribute to quicker convergence during optimization. Additionally, the inherent randomness introduced by these methods can enhance the model's ability to generalize well to new, unseen data, acting as a form of regularization. Furthermore, their memory-efficient nature allows for the processing of large datasets that may not fit into memory at once. Overall, the combination of efficiency, faster convergence, potential for better generalization, and memory efficiency makes SGD and Mini-Batch SGD advantageous choices for training deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Load data.\n",
    "data = pd.read_csv('https://www.dropbox.com/scl/fi/v7iqtlyf3voedweq7xct5/macrodata.csv?rlkey=ccr7auc4i910z2h3xrs7caprn&dl=1',\n",
    "                        index_col = 'Date')\n",
    "\n",
    "# Define target.\n",
    "y = data['Inflation'].iloc[1:]\n",
    "\n",
    "# Define features.\n",
    "X = data[['Inflation', 'Unemployment']].iloc[:-1]\n",
    "\n",
    "# Create train and test sets.\n",
    "y_train, y_test = y.iloc[:400], y.iloc[400:]\n",
    "X_train, X_test = X.iloc[:400], X.iloc[400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 838us/step - loss: 0.1288\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1210\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.1142\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 756us/step - loss: 0.1088\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 840us/step - loss: 0.1043\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 837us/step - loss: 0.1000\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0966\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0935\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0909\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 921us/step - loss: 0.0884\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0868\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 670us/step - loss: 0.0847\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 924us/step - loss: 0.0834\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 929us/step - loss: 0.0818\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0807\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0796\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0786\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 837us/step - loss: 0.0778\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0770\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0763\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 957us/step - loss: 0.0759\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0753\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0748\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0743\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 841us/step - loss: 0.0739\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0735\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 919us/step - loss: 0.0731\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 837us/step - loss: 0.0728\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 920us/step - loss: 0.0725\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0722\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 926us/step - loss: 0.0719\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 922us/step - loss: 0.0717\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 837us/step - loss: 0.0715\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0713\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 757us/step - loss: 0.0710\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0710\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0707\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0705\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0704\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0702\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 926us/step - loss: 0.0701\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0700\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0698\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 923us/step - loss: 0.0697\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0696\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 925us/step - loss: 0.0695\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 664us/step - loss: 0.0695\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0693\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0692\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 839us/step - loss: 0.0692\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 758us/step - loss: 0.0691\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 841us/step - loss: 0.0691\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0690\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 920us/step - loss: 0.0690\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 753us/step - loss: 0.0689\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 670us/step - loss: 0.0689\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0688\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 841us/step - loss: 0.0688\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 670us/step - loss: 0.0688\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 920us/step - loss: 0.0687\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0687\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0686\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0686\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 758us/step - loss: 0.0686\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 920us/step - loss: 0.0685\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 838us/step - loss: 0.0685\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0685\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 837us/step - loss: 0.0685\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0684\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 836us/step - loss: 0.0683\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0684\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 673us/step - loss: 0.0684\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 797us/step - loss: 0.0683\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 838us/step - loss: 0.0684\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0682\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 753us/step - loss: 0.0683\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 937us/step - loss: 0.0682\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0681\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 755us/step - loss: 0.0682\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 756us/step - loss: 0.0681\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 798us/step - loss: 0.0681\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 753us/step - loss: 0.0680\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 755us/step - loss: 0.0679\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0679\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 759us/step - loss: 0.0678\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 757us/step - loss: 0.0678\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 923us/step - loss: 0.0677\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0677\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0677\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 865us/step - loss: 0.0676\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 755us/step - loss: 0.0676\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0676\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 920us/step - loss: 0.0676\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 845us/step - loss: 0.0676\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 925us/step - loss: 0.0676\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 763us/step - loss: 0.0676\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0675\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 796us/step - loss: 0.0676\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 754us/step - loss: 0.0675\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "13/13 [==============================] - 0s 665us/step - loss: 0.0675\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06745272129774094, 0.13329584896564484)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model.\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add input layer.\n",
    "model.add(tf.keras.Input(shape=(2,)))\n",
    "\n",
    "# Define dense layer.\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"relu\", ))\n",
    "\n",
    "# Define output layer.\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "# Print model architecture.\n",
    "print(model.summary())\n",
    "# Evaluate training set using MSE. # Evaluate test set using MSE.\n",
    "model.evaluate(X_train, y_train),model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
